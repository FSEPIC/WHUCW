{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import data_helpers\n",
    "# function: get the node name of ckpt model\n",
    "# from tensorflow.python import pywrap_tensorflow\n",
    "# # checkpoint_path = 'model.ckpt-xxx'\n",
    "# checkpoint_path =  \"../WHUCW/SDGCN/runs/Restaurants/CAtt_GCN_L2/checkpoints/model-3810\"\n",
    "# reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path)\n",
    "# var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "# for key in var_to_shape_map:\n",
    "#     print(\"tensor_name: \", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(400003, 300)\n",
      "400002 400003\n",
      "load word-to-id done!\n",
      "load word-to-id done!\n",
      "load word-to-id done!\n",
      "load word-to-id done!\n",
      "load word-to-id done!\n",
      "load word-to-id done!\n",
      "load targets-to-id done!\n",
      "load targets-to-id done!\n",
      "load targets-to-id done!\n"
     ]
    }
   ],
   "source": [
    "def preprocess(train_file, test_file):\n",
    "    '''\n",
    "    read from the text file.\n",
    "    :return:    sen word id:[324,1413,1,41,43,0,0,0]\n",
    "                sen len:[5]\n",
    "                sen max len :[8]\n",
    "                sen label:[0,0,1]\n",
    "                target word id:[34,154,0,0]\n",
    "                target len: [2]\n",
    "                target max len: [4]\n",
    "                targets word id :[[34,154,0,0],\n",
    "                                  [34,14,12,56],\n",
    "                                  [0,0,0,0]]\n",
    "                targets num = 2\n",
    "                targets len: [2,4,0]\n",
    "                targets max num:[3]\n",
    "                targets_relation_self = [[1,0,0],\n",
    "                                         [0,1,0],\n",
    "                                         [0.0.0]]\n",
    "                targets_relation_cross = [[0,1,0],\n",
    "                                          [1,0,0],\n",
    "                                          [0.0.0]]\n",
    "    '''\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    train_x_str, train_target_str, train_y = data_helpers.load_data_and_labels(train_file)\n",
    "    dev_x_str, dev_target_str, dev_y = data_helpers.load_data_and_labels(test_file)\n",
    "    test_x_str, test_target_str, test_y = data_helpers.load_data_and_labels(test_file)\n",
    "\n",
    "    # word embedding ---> x[324,1413,1,41,43,0,0,0]  y[0,1]\n",
    "    # word_id_mapping,such as  apple--->23 ,w2v  23---->[vector]\n",
    "    word_id_mapping, w2v = data_helpers.load_w2v(\"./glove.6B.300d.txt\", 300)\n",
    "    max_document_length = max([len(x.split(\" \")) for x in (train_x_str + dev_x_str + test_x_str)])\n",
    "    max_target_length = max([len(x.split(\" \")) for x in (train_target_str + dev_target_str + test_target_str)])\n",
    "\n",
    "    # The targets  ---->[[[141,23,45],[23,45,1,2],[2]], ...]\n",
    "    # The number of targets ----> [3, ...]\n",
    "    train_targets_str, train_targets_num = data_helpers.load_targets(train_file)\n",
    "    dev_targets_str, dev_targets_num = data_helpers.load_targets(test_file)\n",
    "    test_targets_str, test_targets_num = data_helpers.load_targets(test_file)\n",
    "    max_target_num = max([len(x) for x in (train_targets_str + test_targets_str)])\n",
    "\n",
    "    # sentence ---> word_id\n",
    "    train_x, train_x_len = data_helpers.word2id(train_x_str, word_id_mapping, max_document_length)\n",
    "    dev_x, dev_x_len = data_helpers.word2id(dev_x_str, word_id_mapping, max_document_length)\n",
    "    test_x, test_x_len = data_helpers.word2id(test_x_str, word_id_mapping, max_document_length)\n",
    "    # target ---> word_id\n",
    "    train_target, train_target_len = data_helpers.word2id(train_target_str, word_id_mapping, max_target_length)\n",
    "    dev_target, dev_target_len = data_helpers.word2id(dev_target_str, word_id_mapping, max_target_length)\n",
    "    test_target, test_target_len = data_helpers.word2id(test_target_str, word_id_mapping, max_target_length)\n",
    "    # targets ---> word_id\n",
    "    train_targets, train_targets_len = data_helpers.word2id_2(train_targets_str, word_id_mapping, max_target_length,\n",
    "                                                              max_target_num)\n",
    "    dev_targets, dev_targets_len = data_helpers.word2id_2(dev_targets_str, word_id_mapping, max_target_length,\n",
    "                                                          max_target_num)\n",
    "    test_targets, test_targets_len = data_helpers.word2id_2(test_targets_str, word_id_mapping, max_target_length,\n",
    "                                                            max_target_num)\n",
    "\n",
    "    # which one targets in all targets\n",
    "    train_target_whichone = data_helpers.get__whichtarget(train_targets_num, max_target_num)\n",
    "    test_target_whichone = data_helpers.get__whichtarget(test_targets_num, max_target_num)\n",
    "    # target position\n",
    "    train_target_position = data_helpers.get_position(train_file, max_document_length)\n",
    "    test_target_position = data_helpers.get_position(test_file, max_document_length)\n",
    "\n",
    "    train_targets_position = data_helpers.get_position_2(train_target_position, train_targets_num, max_target_num)\n",
    "    test_targets_position = data_helpers.get_position_2(test_target_position, test_targets_num, max_target_num)\n",
    "\n",
    "    # Relation Matrix\n",
    "    # use test_target to creat the relation\n",
    "    train_relation_self, train_relation_cross = data_helpers.get_relation(train_targets_num, max_target_num, 'global')\n",
    "    test_relation_self, test_relation_cross = data_helpers.get_relation(test_targets_num, max_target_num, 'global')\n",
    "    Train = {'x': train_x,  # int32(3608, 79)       train sentences input embeddingID\n",
    "             'T': train_target,  # int32(3608, 23)       train target input embeddingID\n",
    "             'Ts': train_targets,  # int32(3608, 13, 23)   train targets input embeddingID\n",
    "             'x_len': train_x_len,  # int32(3608,)          train sentences input len\n",
    "             'T_len': train_target_len,  # int32(3608,)          train target len\n",
    "             'Ts_len': train_targets_len,  # int32(3608, 13)       train targets len\n",
    "             'T_W': train_target_whichone,  # int32(3608, 13)       the ith number of all the targets\n",
    "             'T_P': train_target_position,  # float32(3608, 79)\n",
    "             'Ts_P': train_targets_position,  # float32(3608,13, 79)\n",
    "             'R_Self': train_relation_self,  # int32(3608, 13, 13)\n",
    "             'R_Cross': train_relation_cross,  # int32(3608, 13, 13)\n",
    "             'y': train_y,  # int32(3608, 3)\n",
    "             }\n",
    "    Test = {'x': test_x,\n",
    "            'T': test_target,\n",
    "            'Ts': test_targets,\n",
    "            'x_len': test_x_len,\n",
    "            'T_len': test_target_len,\n",
    "            'Ts_len': test_targets_len,\n",
    "            'T_W': test_target_whichone,\n",
    "            'T_P': test_target_position,\n",
    "            'Ts_P': test_targets_position,\n",
    "            'R_Self': test_relation_self,\n",
    "            'R_Cross': test_relation_cross,\n",
    "            'y': test_y,\n",
    "            }\n",
    "    return Train, Test\n",
    "train, test = preprocess(\"./Restaurants_Train.txt\", \"./Restaurants_Test.txt\")\n",
    "# totext(\"train_\", train)\n",
    "# totext(\"test_\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/PycharmProjects/pythonProject/data_helpers.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  data = np.array(data)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6e14cb0aef77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m#             print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#             \"pre class_id:{}\".format(sess.run(class_id))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mGetSDGoutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./model-8020.pb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-6e14cb0aef77>\u001b[0m in \u001b[0;36mGetSDGoutput\u001b[0;34m(pb_path, Train, Test)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTs_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_len_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT_len_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTs_len_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR_Self_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR_Cross_batxh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT_W_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT_P_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTs_P_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# feed_dict??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             feed_dict = {\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0minput_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0minput_x_1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "def GetSDGoutput(pb_path, Train, Test):\n",
    "    '''\n",
    "    :param pb_path:pb?????\n",
    "    :param image_path:???????\n",
    "    :return:\n",
    "    '''\n",
    "    with tf.Graph().as_default():\n",
    "        output_graph_def = tf.GraphDef()\n",
    "        with open(pb_path, \"rb\") as f:\n",
    "            output_graph_def.ParseFromString(f.read())\n",
    "            tf.import_graph_def(output_graph_def, name=\"\")\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            graph = tf.get_default_graph()\n",
    "            graph_op = graph.get_operations()\n",
    "\n",
    "            # ?????????,???????????\n",
    "            # input:0??????,keep_prob:0??dropout???,?????1,is_training:0????\n",
    "            # \"input_x,input_x_1,input_x_2,sen_len,target_len,\" + \\\n",
    "            # \"targets_all_len/Squeeze,relate_self,relate_cross,\" + \\\n",
    "            # \"which_position,target_position,targets_all_position/Squeeze,\" + \\\n",
    "            # \"input_y,dropout_keep_prob,output/softmax\"\n",
    "            input_x = sess.graph.get_tensor_by_name(\"input_x:0\")\n",
    "            input_x_1 = sess.graph.get_tensor_by_name(\"input_x_1:0\")\n",
    "            input_x_2 = sess.graph.get_tensor_by_name(\"input_x_2:0\")\n",
    "            sen_len = sess.graph.get_tensor_by_name(\"sen_len:0\")\n",
    "            target_len = sess.graph.get_tensor_by_name(\"target_len:0\")\n",
    "            relate_self = sess.graph.get_tensor_by_name(\"relate_self:0\")\n",
    "            relate_cross = sess.graph.get_tensor_by_name(\"relate_cross:0\")\n",
    "            which_position = sess.graph.get_tensor_by_name(\"which_position:0\")\n",
    "            target_position = sess.graph.get_tensor_by_name(\"target_position:0\")\n",
    "            input_y = sess.graph.get_tensor_by_name(\"input_y:0\")\n",
    "            dropout_keep_prob = sess.graph.get_tensor_by_name(\"dropout_keep_prob:0\")\n",
    "            targets_all_position = [sess.graph.get_tensor_by_name(\"targets_all_position/targets_all_position:0\")]\n",
    "            targets_all_len = [sess.graph.get_tensor_by_name(\"targets_all_len/targets_all_len:0\")]\n",
    "\n",
    "            # ?????????\n",
    "            output = sess.graph.get_tensor_by_name(\"output/softmax:0\")\n",
    "            \n",
    "            \n",
    "            batches = data_helpers.batch_iter(\n",
    "                list(zip(Train['x'],Train['T'],Train['Ts'],Train['x_len'], Train['T_len'], Train['Ts_len'],\n",
    "                         Train['R_Self'],Train['R_Cross'],Train['T_W'],Train['T_P'],Train['Ts_P'],Train['y'])), 32, 80)\n",
    "            x_batch,T_batch,Ts_batch,x_len_batch,T_len_batch,Ts_len_batch,R_Self_batch,R_Cross_batxh,T_W_batch,T_P_batch,Ts_P_batch,y_batch = zip(*batches[0])\n",
    "            # feed_dict??\n",
    "            feed_dict = {\n",
    "                input_x: x_batch,\n",
    "                input_x_1: T_batch,\n",
    "                input_x_2: Ts_batch,\n",
    "                sen_len: x_len_batch,\n",
    "                target_len: T_len_batch,\n",
    "                targets_all_len: Ts_len_batch,\n",
    "                relate_self: R_Self_batch,\n",
    "                relate_cross: R_Cross_batxh,\n",
    "                which_position: T_W_batch,\n",
    "                target_position: T_P_batch,\n",
    "                targets_all_position: Ts_P_batch,\n",
    "                input_y: y_batch,\n",
    "                dropout_keep_prob: 0.5\n",
    "            }\n",
    "            return feed_dict\n",
    "            # ??????\n",
    "            # im = read_image(image_path, resize_height, resize_width, normalization=True)\n",
    "            # im = im[np.newaxis, :]\n",
    "            # ?????????????????????????????tensor?????????????\n",
    "            # out=sess.run(\"InceptionV3/Logits/SpatialSqueeze:0\", feed_dict={'input:0': im,'keep_prob:0':1.0,'is_training:0':False})\n",
    "\n",
    "#             out = sess.run(output, feed_dict={input_text: text})\n",
    "#             print(\"out:{}\".format(out))\n",
    "#             score = tf.nn.softmax(out, name='pre')\n",
    "#             class_id = tf.argmax(score, 1)\n",
    "#             print\n",
    "#             \"pre class_id:{}\".format(sess.run(class_id))\n",
    "GetSDGoutput(\"./model-8020.pb\", train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
