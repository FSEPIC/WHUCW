{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# @project : SDGCN\n",
    "# @Author  : plzhao\n",
    "# @Software: PyCharm\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from sklearn import metrics\n",
    "from models.att import Att\n",
    "from models.catt import CAtt\n",
    "from models.att_gcn import Att_GCN\n",
    "from models.catt_gcn import CAtt_GCN_L1,CAtt_GCN_L2,CAtt_GCN_L3,CAtt_GCN_L4,CAtt_GCN_L5,CAtt_GCN_L6,CAtt_GCN_L7,CAtt_GCN_L8\n",
    "from models.catt_gcn_woP import CAtt_GCN_woP\n",
    "# Parameters\n",
    "# ==================================================\n",
    "# \"Restaurants\" or \"laptops\"\n",
    "use_data = \"Restaurants\"\n",
    "# \"Att\"  \"CAtt\"  \"Att_GCN\"  \"CAtt_GCN_L2\"  \"CAtt_GCN_woP\"\n",
    "use_model = \"CAtt_GCN_L2\"\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "datas = {\"Restaurants_train\": \"data/data_res/Restaurants_Train.txt\",\n",
    "         \"Restaurants_test\": \"data/data_res/Restaurants_Test.txt\",\n",
    "         \"Restaurants_embedding\": 'data/data_res/glove.6B.300d.txt',\n",
    "         \"Laptops_train\": \"data/data_lap/Laptops_Train.txt\",\n",
    "         \"Laptops_test\": \"data/data_lap/Laptops_Test.txt\",\n",
    "         \"Laptops_embedding\": 'data/data_lap/Laptops_glove.42B.300d.txt'}\n",
    "#Train model\n",
    "tf.flags.DEFINE_string(\"which_relation\", 'global', \"use which relation.\") #'adjacent','global','rule'\n",
    "tf.flags.DEFINE_string(\"which_model\", use_model, \"Model isused.\")\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_string(\"which_data\", use_data, \"Data is used.\")\n",
    "tf.flags.DEFINE_string(\"train_file\", datas[use_data+\"_train\"], \"Train data source.\")\n",
    "tf.flags.DEFINE_string(\"test_file\", datas[use_data+\"_test\"], \"Test data source.\")\n",
    "\n",
    "#word embedding\n",
    "tf.flags.DEFINE_string('embedding_file_path', datas[use_data+\"_embedding\"], 'embedding file')\n",
    "tf.flags.DEFINE_integer('word_embedding_dim', 300, 'dimension of word embedding')\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 1e-3, \"learning_rate (default: 1e-3)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.01, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch Size (default: 32)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 80, \"Number of training epochs \")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 5, \"Evaluate model on dev set after this many steps \")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 5, \"Save model after this many steps\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 1, \"Number of checkpoints to store\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "# FLAGS._parse_flags()\n",
    "# print(\"\\nParameters:\")\n",
    "# for attr, value in sorted(FLAGS.__flags.items()):\n",
    "#     print(\"{}={}\".format(attr.upper(), value))\n",
    "# print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "read from the text file.\n",
    ":return:    sen word id:[324,1413,1,41,43,0,0,0]\n",
    "            sen len:[5]\n",
    "            sen max len :[8]\n",
    "            sen label:[0,0,1]\n",
    "            target word id:[34,154,0,0]\n",
    "            target len: [2]\n",
    "            target max len: [4]\n",
    "            targets word id :[[34,154,0,0],\n",
    "                              [34,14,12,56],\n",
    "                              [0,0,0,0]]\n",
    "            targets num = 2\n",
    "            targets len: [2,4,0]\n",
    "            targets max num:[3]\n",
    "            targets_relation_self = [[1,0,0],\n",
    "                                     [0,1,0],\n",
    "                                     [0.0.0]]\n",
    "            targets_relation_cross = [[0,1,0],\n",
    "                                      [1,0,0],\n",
    "                                      [0.0.0]]\n",
    "'''\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_x_str,train_target_str, train_y = data_helpers.load_data_and_labels(FLAGS.train_file)\n",
    "dev_x_str,dev_target_str, dev_y = data_helpers.load_data_and_labels(FLAGS.test_file)\n",
    "test_x_str, test_target_str, test_y = data_helpers.load_data_and_labels(FLAGS.test_file)\n",
    "\n",
    "#word embedding ---> x[324,1413,1,41,43,0,0,0]  y[0,1]\n",
    "#word_id_mapping,such as  apple--->23 ,w2v  23---->[vector]\n",
    "word_id_mapping, w2v = data_helpers.load_w2v(FLAGS.embedding_file_path, FLAGS.word_embedding_dim)\n",
    "max_document_length = max([len(x.split(\" \")) for x in (train_x_str + dev_x_str + test_x_str)])\n",
    "max_target_length = max([len(x.split(\" \")) for x in (train_target_str + dev_target_str + test_target_str)])\n",
    "\n",
    "#The targets  ---->[[[141,23,45],[23,45,1,2],[2]], ...]\n",
    "#The number of targets ----> [3, ...]\n",
    "train_targets_str,train_targets_num = data_helpers.load_targets(FLAGS.train_file)\n",
    "dev_targets_str,dev_targets_num = data_helpers.load_targets(FLAGS.test_file)\n",
    "test_targets_str, test_targets_num = data_helpers.load_targets(FLAGS.test_file)\n",
    "max_target_num = max([len(x) for x in (train_targets_str + test_targets_str)])\n",
    "\n",
    "# sentence ---> word_id\n",
    "train_x, train_x_len = data_helpers.word2id(train_x_str,word_id_mapping,max_document_length)\n",
    "dev_x, dev_x_len = data_helpers.word2id(dev_x_str,word_id_mapping,max_document_length)\n",
    "test_x, test_x_len = data_helpers.word2id(test_x_str,word_id_mapping,max_document_length)\n",
    "# target ---> word_id\n",
    "train_target, train_target_len = data_helpers.word2id(train_target_str,word_id_mapping,max_target_length)\n",
    "dev_target, dev_target_len = data_helpers.word2id( dev_target_str,word_id_mapping,max_target_length)\n",
    "test_target, test_target_len = data_helpers.word2id(test_target_str,word_id_mapping,max_target_length)\n",
    "# targets ---> word_id\n",
    "train_targets, train_targets_len = data_helpers.word2id_2(train_targets_str,word_id_mapping,max_target_length,max_target_num)\n",
    "dev_targets, dev_targets_len = data_helpers.word2id_2(dev_targets_str,word_id_mapping,max_target_length,max_target_num)\n",
    "test_targets, test_targets_len = data_helpers.word2id_2(test_targets_str,word_id_mapping,max_target_length,max_target_num)\n",
    "\n",
    "#which one targets in all targets\n",
    "train_target_whichone = data_helpers.get__whichtarget(train_targets_num, max_target_num)\n",
    "test_target_whichone = data_helpers.get__whichtarget(test_targets_num, max_target_num)\n",
    "# target position\n",
    "train_target_position  = data_helpers.get_position(FLAGS.train_file,max_document_length)\n",
    "test_target_position  = data_helpers.get_position(FLAGS.test_file,max_document_length)\n",
    "\n",
    "train_targets_position  = data_helpers.get_position_2(train_target_position,train_targets_num,max_target_num)\n",
    "test_targets_position  = data_helpers.get_position_2(test_target_position,test_targets_num,max_target_num)\n",
    "\n",
    "#Relation Matrix\n",
    "#use test_target to creat the relation\n",
    "train_relation_self,train_relation_cross = data_helpers.get_relation(train_targets_num, max_target_num,FLAGS.which_relation)\n",
    "test_relation_self, test_relation_cross = data_helpers.get_relation(test_targets_num, max_target_num,FLAGS.which_relation)\n",
    "Train = {'x':train_x,                       # int32(3608, 79)       train sentences input embeddingID\n",
    "         'T':train_target,                  # int32(3608, 23)       train target input embeddingID\n",
    "         'Ts':train_targets,                # int32(3608, 13, 23)   train targets input embeddingID\n",
    "         'x_len':train_x_len,               # int32(3608,)          train sentences input len\n",
    "         'T_len':train_target_len,          # int32(3608,)          train target len\n",
    "         'Ts_len': train_targets_len,       # int32(3608, 13)       train targets len\n",
    "         'T_W': train_target_whichone,      # int32(3608, 13)       the ith number of all the targets\n",
    "         'T_P':train_target_position,       # float32(3608, 79)\n",
    "         'Ts_P': train_targets_position,    # float32(3608,13, 79)\n",
    "         'R_Self': train_relation_self,     # int32(3608, 13, 13)\n",
    "         'R_Cross': train_relation_cross,   # int32(3608, 13, 13)\n",
    "         'y': train_y,  # int32(3608, 3)\n",
    "        }\n",
    "Test = { 'x':test_x,\n",
    "         'T':test_target,\n",
    "         'Ts':test_targets,\n",
    "         'x_len':test_x_len,\n",
    "         'T_len':test_target_len,\n",
    "         'Ts_len': test_targets_len,\n",
    "         'T_W': test_target_whichone,\n",
    "         'T_P': test_target_position,\n",
    "         'Ts_P': test_targets_position,\n",
    "         'R_Self': test_relation_self,\n",
    "         'R_Cross': test_relation_cross,\n",
    "         'y': test_y,\n",
    "        }\n",
    "#\n",
    "# batches = data_helpers.batch_iter(\n",
    "#     list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(word_id_mapping)))\n",
    "print(\"Train/Dev/test split: {:d}/{:d}/{:d}\".format(len(train_y), len(dev_y), len(test_y)))\n",
    "word_embedding = w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def freeze_graph(input_checkpoint,output_graph):\n",
    "    '''\n",
    "    :param input_checkpoint:\n",
    "    :param output_graph: PB模型保存路径\n",
    "    :return:\n",
    "    '''\n",
    "    # checkpoint = tf.train.get_checkpoint_state(model_folder) #检查目录下ckpt文件状态是否可用\n",
    "    # input_checkpoint = checkpoint.model_checkpoint_path #得ckpt文件路径\n",
    " \n",
    "    # 指定输出的节点名称,该节点名称必须是原模型中存在的节点\n",
    "    output_node_names = \"output/Variable:0,output/Variable_1:0\"\n",
    "    tf.disable_v2_behavior()\n",
    "    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=True)\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, input_checkpoint) #恢复图并得到数据\n",
    "        output_graph_def = graph_util.convert_variables_to_constants(  # 模型持久化，将变量值固定\n",
    "            sess=sess,\n",
    "            input_graph_def=sess.graph_def,# 等于:sess.graph_def\n",
    "            output_node_names=output_node_names.split(\",\"))# 如果有多个输出节点，以逗号隔开\n",
    " \n",
    "        with tf.gfile.GFile(output_graph, \"wb\") as f: #保存模型\n",
    "            f.write(output_graph_def.SerializeToString()) #序列化输出\n",
    "        print(\"%d ops in the final graph.\" % len(output_graph_def.node)) #得到当前图有几个操作节点\n",
    "\n",
    "\n",
    "def train(Train, Test, word_embedding):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "          log_device_placement=FLAGS.log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            model = eval(use_model)(\n",
    "                sequence_length=Train['x'].shape[1],\n",
    "                target_sequence_length = Train['T'].shape[1],\n",
    "                targets_num_max = Train['Ts'].shape[1],\n",
    "                num_classes=Train['y'].shape[1],\n",
    "                word_embedding = word_embedding,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "            writer = tf.summary.FileWriter(\"logs/LSTM_GCN3\", sess.graph)\n",
    "\n",
    "            vs = tf.trainable_variables()\n",
    "            print('There are %d train_able_variables in the Graph: ' % len(vs))\n",
    "            for v in vs:\n",
    "                print(v)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(model.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", use_data,use_model))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", model.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", model.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Test summaries\n",
    "            test_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            test_summary_dir = os.path.join(out_dir, \"summaries\", \"test\")\n",
    "            test_summary_writer = tf.summary.FileWriter(test_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            # else:\n",
    "            #     raise Exception('The checkpoint_dir already exists:',checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    " \n",
    "            # Write vocabulary\n",
    "            # vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            # Initialize all variables\n",
    "#             sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            saver.restore(sess, \"./runs/Restaurants/CAtt_GCN_L2/checkpoints/model-500\")\n",
    "            def log(string):\n",
    "                file = open('./log.txt','a+')\n",
    "                file.write(string+\"\\n\")\n",
    "                file.close()\n",
    "            def train_step(x_batch,T_batch,Ts_batch,x_len_batch,T_len_batch,Ts_len_batch,R_Self_batch,R_Cross_batxh,T_W_batch,T_P_batch,Ts_P_batch,y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                    model.input_x: x_batch,\n",
    "                    model.input_target:T_batch,\n",
    "                    model.input_targets_all:Ts_batch,\n",
    "                    model.sen_len:x_len_batch,\n",
    "                    model.target_len:T_len_batch,\n",
    "                    model.targets_all_len_a:Ts_len_batch,\n",
    "                    model.relate_self:R_Self_batch,\n",
    "                    model.relate_cross:R_Cross_batxh,\n",
    "                    model.target_which:T_W_batch,\n",
    "                    model.target_position: T_P_batch,\n",
    "                    model.targets_all_position_a: Ts_P_batch,\n",
    "                    model.input_y: y_batch,\n",
    "                    model.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, model.loss, model.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(vs[19].eval())\n",
    "                log(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            def test_step(x_batch,T_batch,Ts_batch,x_len_batch,T_len_batch,Ts_len_batch,R_Self_batch,R_Cross_batxh,T_W_batch,T_P_batch,Ts_P_batch,y_batch, summary = None,writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                    model.input_x: x_batch,\n",
    "                    model.input_target:T_batch,\n",
    "                    model.input_targets_all:Ts_batch,\n",
    "                    model.sen_len:x_len_batch,\n",
    "                    model.target_len:T_len_batch,\n",
    "                    model.targets_all_len_a:Ts_len_batch,\n",
    "                    model.relate_self:R_Self_batch,\n",
    "                    model.relate_cross:R_Cross_batxh,\n",
    "                    model.target_which: T_W_batch,\n",
    "                    model.target_position: T_P_batch,\n",
    "                    model.targets_all_position_a: Ts_P_batch,\n",
    "                    model.input_y: y_batch,\n",
    "                    model.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy, softmax,true_y,predictions = sess.run(\n",
    "                    [global_step, summary, model.loss, model.accuracy, model.softmax,model.true_y, model.predictions],\n",
    "                    feed_dict)\n",
    "                F1 = metrics.f1_score(true_y, predictions, average='macro')\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(vs[19].eval())\n",
    "                log(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy,F1))\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy,F1))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "                return accuracy,softmax,F1\n",
    "\n",
    "            # Generate batches\n",
    "            batches = data_helpers.batch_iter(\n",
    "                list(zip(Train['x'],Train['T'],Train['Ts'],Train['x_len'], Train['T_len'], Train['Ts_len'],\n",
    "                         Train['R_Self'],Train['R_Cross'],Train['T_W'],Train['T_P'],Train['Ts_P'],Train['y'])), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            # Training loop. For each batch...\n",
    "            train_acc, dev_acc, test_acc, train_all_softmax, test_all_softmax = [], [], [], [], []\n",
    "            max_test_acc = 0\n",
    "            max_test_F1_macro = 0\n",
    "            for batch in batches:\n",
    "                x_batch,T_batch,Ts_batch,x_len_batch,T_len_batch,Ts_len_batch,R_Self_batch,R_Cross_batxh,T_W_batch,T_P_batch,Ts_P_batch,y_batch = zip(*batch)\n",
    "                train_step(x_batch,T_batch,Ts_batch,x_len_batch,T_len_batch,Ts_len_batch,R_Self_batch,R_Cross_batxh,T_W_batch,T_P_batch,Ts_P_batch,y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    log('\\nBy now ,the max test acc is: ')\n",
    "                    print('\\nBy now ,the max test acc is: ', max_test_acc)\n",
    "                    log('        the max F1 score is: ')\n",
    "                    print('        the max F1 score is: ', max_test_F1_macro)\n",
    "                    log(\"\\nEvaluation Text:\")\n",
    "                    print(\"\\nEvaluation Text:\")\n",
    "                    test_acc_i, test_softmax_i, test_F1_i = test_step(Test['x'],Test['T'],Test['Ts'],Test['x_len'], Test['T_len'], Test['Ts_len'],\n",
    "                                                           Test['R_Self'],Test['R_Cross'],Test['T_W'],Test['T_P'],Test['Ts_P'],Test['y'], summary = test_summary_op, writer=test_summary_writer)\n",
    "                    test_acc.append(test_acc_i)\n",
    "                    test_all_softmax.append(test_softmax_i)\n",
    "                    log('----------------------------------------------------------')\n",
    "                    print('----------------------------------------------------------')\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    if test_acc_i>max_test_acc:\n",
    "                        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                        # 输入ckpt模型路径\n",
    "#                         input_checkpoint=path\n",
    "                        # 输出pb模型的路径\n",
    "                        out_pb_path=\"./pb/frozen_model.pb\"\n",
    "                        # 调用freeze_graph将ckpt转为pb\n",
    "#                         freeze_graph(str(path),out_pb_path)\n",
    "#                         tf.train.write_graph(sess.graph_def, './pb', 'model.pb')\n",
    "#                         freeze_graph.freeze_graph(\n",
    "#                             input_graph='./pb/model.pb',\n",
    "#                             input_saver='',\n",
    "#                             input_binary=False, \n",
    "#                             input_checkpoint=path, \n",
    "#                             output_node_names='output/Variable_1:0',\n",
    "#                             restore_op_name='save/restore_all',\n",
    "#                             filename_tensor_name='save/Const:0',\n",
    "#                             output_graph='./pb/frozen_model.pb',\n",
    "#                             clear_devices=False,\n",
    "#                             initializer_nodes=''\n",
    "#                         )\n",
    "                        log(\"Saved model checkpoint to {}\\n\".format(path)+'\\n'+'->>>>>>>>>>>>>>>>>>>>>>>')\n",
    "                        print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                        print('->>>>>>>>>>>>>>>>>>>>>>>')\n",
    "                        max_test_step = current_step\n",
    "                        max_test_acc = test_acc_i\n",
    "                    if test_F1_i > max_test_F1_macro:\n",
    "                        max_test_F1_macro = test_F1_i\n",
    "            log('max_test_step: ' + str(max_test_step))\n",
    "            print('max_test_step: ', max_test_step)\n",
    "            log('max_test_acc: ' + str(max_test_acc))\n",
    "            print('max_test_acc: ', max_test_acc)\n",
    "            log('max_test_F1_macro: ' + str(max_test_F1_macro))\n",
    "            print('max_test_F1_Rv8xCxELmacro: ', max_test_F1_macro)\n",
    "    return train_acc, dev_acc, max_test_acc,max_test_F1_macro,max_test_step, train_all_softmax, test_all_softmax\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t#模型训练\n",
    "\ttrain_acc, dev_acc, max_test_acc,max_test_F1_macro,max_test_step, train_all_softmax, test_all_softmax = train(Train, Test, word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./runs/Restaurants/CAtt_GCN_L2/checkpoints/model-1110\n",
      "2 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import graph_util\n",
    "def freeze_graph(input_checkpoint,output_graph):\n",
    "    '''\n",
    "    :param input_checkpoint:\n",
    "    :param output_graph: PB模型保存路径\n",
    "    :return:\n",
    "    '''\n",
    "    # checkpoint = tf.train.get_checkpoint_state(model_folder) #检查目录下ckpt文件状态是否可用\n",
    "    # input_checkpoint = checkpoint.model_checkpoint_path #得ckpt文件路径\n",
    " \n",
    "    # 指定输出的节点名称,该节点名称必须是原模型中存在的节点\n",
    "    output_node_names = \"output/Variable,output/Variable_1\"\n",
    "    tf.disable_v2_behavior()\n",
    "    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=True)\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, input_checkpoint) #恢复图并得到数据\n",
    "        output_graph_def = graph_util.convert_variables_to_constants(  # 模型持久化，将变量值固定\n",
    "            sess=sess,\n",
    "            input_graph_def=sess.graph_def,# 等于:sess.graph_def\n",
    "            output_node_names=output_node_names.split(\",\"))# 如果有多个输出节点，以逗号隔开\n",
    " \n",
    "        with tf.gfile.GFile(output_graph, \"wb\") as f: #保存模型\n",
    "            f.write(output_graph_def.SerializeToString()) #序列化输出\n",
    "        print(\"%d ops in the final graph.\" % len(output_graph_def.node)) #得到当前图有几个操作节点\n",
    "freeze_graph(\"./runs/Restaurants/CAtt_GCN_L2/checkpoints/model-1110\",\"./pb/model.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_file = 'data/data_res/300_rest14_embedding_matrix.pkl'\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
