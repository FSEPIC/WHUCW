{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f672b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# @project : SDGCN\n",
    "# @Author  : plzhao\n",
    "# @Software: PyCharm\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from sklearn import metrics\n",
    "from models.att import Att\n",
    "from models.catt import CAtt\n",
    "from models.att_gcn import Att_GCN\n",
    "from models.catt_gcn import CAtt_GCN_L1,CAtt_GCN_L2,CAtt_GCN_L3,CAtt_GCN_L4,CAtt_GCN_L5,CAtt_GCN_L6,CAtt_GCN_L7,CAtt_GCN_L8\n",
    "from models.catt_gcn_woP import CAtt_GCN_woP\n",
    "# Parameters\n",
    "# ==================================================\n",
    "# \"Restaurants\" or \"laptops\"\n",
    "use_data = \"Restaurants\"\n",
    "# \"Att\"  \"CAtt\"  \"Att_GCN\"  \"CAtt_GCN_L2\"  \"CAtt_GCN_woP\"\n",
    "use_model = \"CAtt_GCN_L2\"\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "datas = {\"Restaurants_train\": \"data/data_res/Restaurants_Train.txt\",\n",
    "         \"Restaurants_test\": \"data/data_res/Restaurants_Test.txt\",\n",
    "         \"Restaurants_embedding\": 'data/data_res/300_rest14_embedding_matrix.pkl',\n",
    "         \"Laptops_train\": \"data/data_lap/Laptops_Train.txt\",\n",
    "         \"Laptops_test\": \"data/data_lap/Laptops_Test.txt\",\n",
    "         \"Laptops_embedding\": 'data/data_lap/Laptops_glove.42B.300d.txt'}\n",
    "#Train model\n",
    "tf.flags.DEFINE_string(\"which_relation\", 'global', \"use which relation.\") #'adjacent','global','rule'\n",
    "tf.flags.DEFINE_string(\"which_model\", use_model, \"Model isused.\")\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_string(\"which_data\", use_data, \"Data is used.\")\n",
    "tf.flags.DEFINE_string(\"train_file\", datas[use_data+\"_train\"], \"Train data source.\")\n",
    "tf.flags.DEFINE_string(\"test_file\", datas[use_data+\"_test\"], \"Test data source.\")\n",
    "\n",
    "#word embedding\n",
    "tf.flags.DEFINE_string('embedding_file_path', datas[use_data+\"_embedding\"], 'embedding file')\n",
    "tf.flags.DEFINE_integer('word_embedding_dim', 300, 'dimension of word embedding')\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 1e-3, \"learning_rate (default: 1e-3)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.01, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch Size (default: 32)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 80, \"Number of training epochs \")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 5, \"Evaluate model on dev set after this many steps \")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 5, \"Save model after this many steps\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 1, \"Number of checkpoints to store\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "# FLAGS._parse_flags()\n",
    "# print(\"\\nParameters:\")\n",
    "# for attr, value in sorted(FLAGS.__flags.items()):\n",
    "#     print(\"{}={}\".format(attr.upper(), value))\n",
    "# print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28785753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(4587, 300)\n",
      "4586 4587\n",
      "load word-to-id done!\n",
      "load word-to-id done!\n",
      "load word-to-id done!\n",
      "load word-to-id done!\n",
      "load word-to-id done!\n",
      "load word-to-id done!\n",
      "load targets-to-id done!\n",
      "load targets-to-id done!\n",
      "load targets-to-id done!\n",
      "Vocabulary Size: 4305\n",
      "Train/Dev/test split: 3608/1120/1120\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "read from the text file.\n",
    ":return:    sen word id:[324,1413,1,41,43,0,0,0]\n",
    "            sen len:[5]\n",
    "            sen max len :[8]\n",
    "            sen label:[0,0,1]\n",
    "            target word id:[34,154,0,0]\n",
    "            target len: [2]\n",
    "            target max len: [4]\n",
    "            targets word id :[[34,154,0,0],\n",
    "                              [34,14,12,56],\n",
    "                              [0,0,0,0]]\n",
    "            targets num = 2\n",
    "            targets len: [2,4,0]\n",
    "            targets max num:[3]\n",
    "            targets_relation_self = [[1,0,0],\n",
    "                                     [0,1,0],\n",
    "                                     [0.0.0]]\n",
    "            targets_relation_cross = [[0,1,0],\n",
    "                                      [1,0,0],\n",
    "                                      [0.0.0]]\n",
    "'''\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_x_str,train_target_str, train_y = data_helpers.load_data_and_labels(FLAGS.train_file)\n",
    "dev_x_str,dev_target_str, dev_y = data_helpers.load_data_and_labels(FLAGS.test_file)\n",
    "test_x_str, test_target_str, test_y = data_helpers.load_data_and_labels(FLAGS.test_file)\n",
    "\n",
    "#word embedding ---> x[324,1413,1,41,43,0,0,0]  y[0,1]\n",
    "#word_id_mapping,such as  apple--->23 ,w2v  23---->[vector]\n",
    "word_id_mapping, w2v = data_helpers.load_w2v(FLAGS.embedding_file_path, FLAGS.word_embedding_dim)\n",
    "max_document_length = max([len(x.split(\" \")) for x in (train_x_str + dev_x_str + test_x_str)])\n",
    "max_target_length = max([len(x.split(\" \")) for x in (train_target_str + dev_target_str + test_target_str)])\n",
    "\n",
    "#The targets  ---->[[[141,23,45],[23,45,1,2],[2]], ...]\n",
    "#The number of targets ----> [3, ...]\n",
    "train_targets_str,train_targets_num = data_helpers.load_targets(FLAGS.train_file)\n",
    "dev_targets_str,dev_targets_num = data_helpers.load_targets(FLAGS.test_file)\n",
    "test_targets_str, test_targets_num = data_helpers.load_targets(FLAGS.test_file)\n",
    "max_target_num = max([len(x) for x in (train_targets_str + test_targets_str)])\n",
    "\n",
    "# sentence ---> word_id\n",
    "train_x, train_x_len = data_helpers.word2id(train_x_str,word_id_mapping,max_document_length)\n",
    "dev_x, dev_x_len = data_helpers.word2id(dev_x_str,word_id_mapping,max_document_length)\n",
    "test_x, test_x_len = data_helpers.word2id(test_x_str,word_id_mapping,max_document_length)\n",
    "# target ---> word_id\n",
    "train_target, train_target_len = data_helpers.word2id(train_target_str,word_id_mapping,max_target_length)\n",
    "dev_target, dev_target_len = data_helpers.word2id( dev_target_str,word_id_mapping,max_target_length)\n",
    "test_target, test_target_len = data_helpers.word2id(test_target_str,word_id_mapping,max_target_length)\n",
    "# targets ---> word_id\n",
    "train_targets, train_targets_len = data_helpers.word2id_2(train_targets_str,word_id_mapping,max_target_length,max_target_num)\n",
    "dev_targets, dev_targets_len = data_helpers.word2id_2(dev_targets_str,word_id_mapping,max_target_length,max_target_num)\n",
    "test_targets, test_targets_len = data_helpers.word2id_2(test_targets_str,word_id_mapping,max_target_length,max_target_num)\n",
    "\n",
    "#which one targets in all targets\n",
    "train_target_whichone = data_helpers.get__whichtarget(train_targets_num, max_target_num)\n",
    "test_target_whichone = data_helpers.get__whichtarget(test_targets_num, max_target_num)\n",
    "# target position\n",
    "train_target_position  = data_helpers.get_position(FLAGS.train_file,max_document_length)\n",
    "test_target_position  = data_helpers.get_position(FLAGS.test_file,max_document_length)\n",
    "\n",
    "train_targets_position  = data_helpers.get_position_2(train_target_position,train_targets_num,max_target_num)\n",
    "test_targets_position  = data_helpers.get_position_2(test_target_position,test_targets_num,max_target_num)\n",
    "\n",
    "#Relation Matrix\n",
    "#use test_target to creat the relation\n",
    "train_relation_self,train_relation_cross = data_helpers.get_relation(train_targets_num, max_target_num,FLAGS.which_relation)\n",
    "test_relation_self, test_relation_cross = data_helpers.get_relation(test_targets_num, max_target_num,FLAGS.which_relation)\n",
    "Train = {'x':train_x,                       # int32(3608, 79)       train sentences input embeddingID\n",
    "         'T':train_target,                  # int32(3608, 23)       train target input embeddingID\n",
    "         'Ts':train_targets,                # int32(3608, 13, 23)   train targets input embeddingID\n",
    "         'x_len':train_x_len,               # int32(3608,)          train sentences input len\n",
    "         'T_len':train_target_len,          # int32(3608,)          train target len\n",
    "         'Ts_len': train_targets_len,       # int32(3608, 13)       train targets len\n",
    "         'T_W': train_target_whichone,      # int32(3608, 13)       the ith number of all the targets\n",
    "         'T_P':train_target_position,       # float32(3608, 79)\n",
    "         'Ts_P': train_targets_position,    # float32(3608,13, 79)\n",
    "         'R_Self': train_relation_self,     # int32(3608, 13, 13)\n",
    "         'R_Cross': train_relation_cross,   # int32(3608, 13, 13)\n",
    "         'y': train_y,  # int32(3608, 3)\n",
    "        }\n",
    "Test = { 'x':test_x,\n",
    "         'T':test_target,\n",
    "         'Ts':test_targets,\n",
    "         'x_len':test_x_len,\n",
    "         'T_len':test_target_len,\n",
    "         'Ts_len': test_targets_len,\n",
    "         'T_W': test_target_whichone,\n",
    "         'T_P': test_target_position,\n",
    "         'Ts_P': test_targets_position,\n",
    "         'R_Self': test_relation_self,\n",
    "         'R_Cross': test_relation_cross,\n",
    "         'y': test_y,\n",
    "        }\n",
    "#\n",
    "# batches = data_helpers.batch_iter(\n",
    "#     list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(word_id_mapping)))\n",
    "print(\"Train/Dev/test split: {:d}/{:d}/{:d}\".format(len(train_y), len(dev_y), len(test_y)))\n",
    "word_embedding = w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe090d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/epicfs/Documents/project/WHUGCNSC/SDGCN/models/catt_gcn.py:163: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/epicfs/Documents/project/WHUGCNSC/SDGCN/models/catt_gcn.py:200: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "embedding_size 300\n",
      "WARNING:tensorflow:From /Users/epicfs/Documents/project/WHUGCNSC/SDGCN/models/nn_layer.py:52: The name tf.nn.rnn_cell.DropoutWrapper is deprecated. Please use tf.compat.v1.nn.rnn_cell.DropoutWrapper instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/epicfs/Documents/project/WHUGCNSC/SDGCN/models/nn_layer.py:52: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /Users/epicfs/Documents/project/WHUGCNSC/SDGCN/models/nn_layer.py:63: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/epicfs/opt/anaconda3/envs/whusdg/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/epicfs/opt/anaconda3/envs/whusdg/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Users/epicfs/opt/anaconda3/envs/whusdg/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/epicfs/opt/anaconda3/envs/whusdg/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/epicfs/Documents/project/WHUGCNSC/SDGCN/models/nn_layer.py:134: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/epicfs/Documents/project/WHUGCNSC/SDGCN/models/catt_gcn.py:232: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/epicfs/Documents/project/WHUGCNSC/SDGCN/models/att_layer.py:34: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/epicfs/Documents/project/WHUGCNSC/SDGCN/models/att_layer.py:11: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/epicfs/Documents/project/WHUGCNSC/SDGCN/models/catt_gcn.py:296: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/epicfs/Documents/project/WHUGCNSC/SDGCN/models/catt_gcn.py:298: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/epicfs/Documents/project/WHUGCNSC/SDGCN/models/catt_gcn.py:306: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "LOADED Att-GCN!\n",
      "There are 20 train_able_variables in the Graph: \n",
      "<tf.Variable 'bi-lstm-sentence/fw/lstm_cell/kernel:0' shape=(600, 1200) dtype=float32_ref>\n",
      "<tf.Variable 'bi-lstm-sentence/fw/lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>\n",
      "<tf.Variable 'bi-lstm-sentence/bw/lstm_cell/kernel:0' shape=(600, 1200) dtype=float32_ref>\n",
      "<tf.Variable 'bi-lstm-sentence/bw/lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>\n",
      "<tf.Variable 'Bi-LSTM_targets/bi-lstm-targets/fw/lstm_cell/kernel:0' shape=(600, 1200) dtype=float32_ref>\n",
      "<tf.Variable 'Bi-LSTM_targets/bi-lstm-targets/fw/lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>\n",
      "<tf.Variable 'Bi-LSTM_targets/bi-lstm-targets/bw/lstm_cell/kernel:0' shape=(600, 1200) dtype=float32_ref>\n",
      "<tf.Variable 'Bi-LSTM_targets/bi-lstm-targets/bw/lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>\n",
      "<tf.Variable 'Attention-targets_all2sentence/att_w_tar:0' shape=(600, 600) dtype=float32_ref>\n",
      "<tf.Variable 'Attention-targets_all2sentence/att_w_sen:0' shape=(600, 600) dtype=float32_ref>\n",
      "<tf.Variable 'GCN_layer1/W_cross:0' shape=(600, 600) dtype=float32_ref>\n",
      "<tf.Variable 'GCN_layer1/b_cross:0' shape=(600,) dtype=float32_ref>\n",
      "<tf.Variable 'GCN_layer1/W_self:0' shape=(600, 600) dtype=float32_ref>\n",
      "<tf.Variable 'GCN_layer1/b_self:0' shape=(600,) dtype=float32_ref>\n",
      "<tf.Variable 'GCN_layer2/W_cross:0' shape=(600, 600) dtype=float32_ref>\n",
      "<tf.Variable 'GCN_layer2/b_cross:0' shape=(600,) dtype=float32_ref>\n",
      "<tf.Variable 'GCN_layer2/W_self:0' shape=(600, 600) dtype=float32_ref>\n",
      "<tf.Variable 'GCN_layer2/b_self:0' shape=(600,) dtype=float32_ref>\n",
      "<tf.Variable 'output/Variable:0' shape=(600, 3) dtype=float32_ref>\n",
      "<tf.Variable 'output/Variable_1:0' shape=(3,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name bi-lstm-sentence/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm-sentence/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm-sentence/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm-sentence/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm-sentence/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm-sentence/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm-sentence/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm-sentence/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm-sentence/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm-sentence/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm-sentence/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm-sentence/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm-sentence/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm-sentence/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm-sentence/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm-sentence/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM_targets/bi-lstm-targets/fw/lstm_cell/kernel:0/grad/hist is illegal; using Bi-LSTM_targets/bi-lstm-targets/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM_targets/bi-lstm-targets/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using Bi-LSTM_targets/bi-lstm-targets/fw/lstm_cell/kernel_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Bi-LSTM_targets/bi-lstm-targets/fw/lstm_cell/bias:0/grad/hist is illegal; using Bi-LSTM_targets/bi-lstm-targets/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM_targets/bi-lstm-targets/fw/lstm_cell/bias:0/grad/sparsity is illegal; using Bi-LSTM_targets/bi-lstm-targets/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM_targets/bi-lstm-targets/bw/lstm_cell/kernel:0/grad/hist is illegal; using Bi-LSTM_targets/bi-lstm-targets/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM_targets/bi-lstm-targets/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using Bi-LSTM_targets/bi-lstm-targets/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM_targets/bi-lstm-targets/bw/lstm_cell/bias:0/grad/hist is illegal; using Bi-LSTM_targets/bi-lstm-targets/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM_targets/bi-lstm-targets/bw/lstm_cell/bias:0/grad/sparsity is illegal; using Bi-LSTM_targets/bi-lstm-targets/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Attention-targets_all2sentence/att_w_tar:0/grad/hist is illegal; using Attention-targets_all2sentence/att_w_tar_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Attention-targets_all2sentence/att_w_tar:0/grad/sparsity is illegal; using Attention-targets_all2sentence/att_w_tar_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Attention-targets_all2sentence/att_w_sen:0/grad/hist is illegal; using Attention-targets_all2sentence/att_w_sen_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Attention-targets_all2sentence/att_w_sen:0/grad/sparsity is illegal; using Attention-targets_all2sentence/att_w_sen_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name GCN_layer1/W_cross:0/grad/hist is illegal; using GCN_layer1/W_cross_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name GCN_layer1/W_cross:0/grad/sparsity is illegal; using GCN_layer1/W_cross_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name GCN_layer1/b_cross:0/grad/hist is illegal; using GCN_layer1/b_cross_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name GCN_layer1/b_cross:0/grad/sparsity is illegal; using GCN_layer1/b_cross_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name GCN_layer1/W_self:0/grad/hist is illegal; using GCN_layer1/W_self_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name GCN_layer1/W_self:0/grad/sparsity is illegal; using GCN_layer1/W_self_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name GCN_layer1/b_self:0/grad/hist is illegal; using GCN_layer1/b_self_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name GCN_layer1/b_self:0/grad/sparsity is illegal; using GCN_layer1/b_self_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name GCN_layer2/W_cross:0/grad/hist is illegal; using GCN_layer2/W_cross_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name GCN_layer2/W_cross:0/grad/sparsity is illegal; using GCN_layer2/W_cross_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name GCN_layer2/b_cross:0/grad/hist is illegal; using GCN_layer2/b_cross_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name GCN_layer2/b_cross:0/grad/sparsity is illegal; using GCN_layer2/b_cross_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name GCN_layer2/W_self:0/grad/hist is illegal; using GCN_layer2/W_self_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name GCN_layer2/W_self:0/grad/sparsity is illegal; using GCN_layer2/W_self_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name GCN_layer2/b_self:0/grad/hist is illegal; using GCN_layer2/b_self_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name GCN_layer2/b_self:0/grad/sparsity is illegal; using GCN_layer2/b_self_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/Variable:0/grad/hist is illegal; using output/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/Variable:0/grad/sparsity is illegal; using output/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/Variable_1:0/grad/hist is illegal; using output/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/Variable_1:0/grad/sparsity is illegal; using output/Variable_1_0/grad/sparsity instead.\n",
      "Writing to /Users/epicfs/Documents/project/WHUGCNSC/SDGCN/runs/Restaurants/CAtt_GCN_L2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epicfs/Documents/project/WHUGCNSC/SDGCN/data_helpers.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  data = np.array(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-09T18:16:53.249478: step 1, loss 10.3965, acc 0.1875\n",
      "2022-02-09T18:16:54.760923: step 2, loss 15.7729, acc 0.625\n",
      "2022-02-09T18:16:55.696153: step 3, loss 11.5163, acc 0.5\n",
      "2022-02-09T18:16:56.598404: step 4, loss 10.1048, acc 0.375\n",
      "2022-02-09T18:16:57.711230: step 5, loss 10.1289, acc 0.5625\n",
      "\n",
      "By now ,the max test acc is:  0\n",
      "        the max F1 score is:  0\n",
      "\n",
      "Evaluation Text:\n",
      "2022-02-09T18:17:34.547241: step 5, loss 9.96448, acc 0.542857\n",
      "----------------------------------------------------------\n",
      "\n",
      "Saved model checkpoint to /Users/epicfs/Documents/project/WHUGCNSC/SDGCN/runs/Restaurants/CAtt_GCN_L2/checkpoints/model-5\n",
      "\n",
      "->>>>>>>>>>>>>>>>>>>>>>>\n",
      "2022-02-09T18:17:39.180195: step 6, loss 10.0441, acc 0.5625\n",
      "2022-02-09T18:17:40.317121: step 7, loss 9.8435, acc 0.53125\n",
      "2022-02-09T18:17:41.710010: step 8, loss 10.0448, acc 0.65625\n",
      "2022-02-09T18:17:42.769794: step 9, loss 10.3723, acc 0.40625\n",
      "2022-02-09T18:17:43.899189: step 10, loss 9.87437, acc 0.59375\n",
      "\n",
      "By now ,the max test acc is:  0.54285717\n",
      "        the max F1 score is:  0.31881756257955873\n",
      "\n",
      "Evaluation Text:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(Train, Test, word_embedding):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "          log_device_placement=FLAGS.log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            model = eval(use_model)(\n",
    "                sequence_length=Train['x'].shape[1],\n",
    "                target_sequence_length = Train['T'].shape[1],\n",
    "                targets_num_max = Train['Ts'].shape[1],\n",
    "                num_classes=Train['y'].shape[1],\n",
    "                word_embedding = word_embedding,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "            writer = tf.summary.FileWriter(\"logs/LSTM_GCN3\", sess.graph)\n",
    "\n",
    "            vs = tf.trainable_variables()\n",
    "            print('There are %d train_able_variables in the Graph: ' % len(vs))\n",
    "            for v in vs:\n",
    "                print(v)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(model.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", use_data,use_model))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", model.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", model.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Test summaries\n",
    "            test_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            test_summary_dir = os.path.join(out_dir, \"summaries\", \"test\")\n",
    "            test_summary_writer = tf.summary.FileWriter(test_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            # else:\n",
    "            #     raise Exception('The checkpoint_dir already exists:',checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "            # Write vocabulary\n",
    "            # vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            def log(string):\n",
    "                file = open('./log.txt','a+')\n",
    "                file.write(string+\"\\n\")\n",
    "                file.close()\n",
    "            def train_step(x_batch,T_batch,Ts_batch,x_len_batch,T_len_batch,Ts_len_batch,R_Self_batch,R_Cross_batxh,T_W_batch,T_P_batch,Ts_P_batch,y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                    model.input_x: x_batch,\n",
    "                    model.input_target:T_batch,\n",
    "                    model.input_targets_all:Ts_batch,\n",
    "                    model.sen_len:x_len_batch,\n",
    "                    model.target_len:T_len_batch,\n",
    "                    model.targets_all_len_a:Ts_len_batch,\n",
    "                    model.relate_self:R_Self_batch,\n",
    "                    model.relate_cross:R_Cross_batxh,\n",
    "                    model.target_which:T_W_batch,\n",
    "                    model.target_position: T_P_batch,\n",
    "                    model.targets_all_position_a: Ts_P_batch,\n",
    "                    model.input_y: y_batch,\n",
    "                    model.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, model.loss, model.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                log(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            def test_step(x_batch,T_batch,Ts_batch,x_len_batch,T_len_batch,Ts_len_batch,R_Self_batch,R_Cross_batxh,T_W_batch,T_P_batch,Ts_P_batch,y_batch, summary = None,writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                    model.input_x: x_batch,\n",
    "                    model.input_target:T_batch,\n",
    "                    model.input_targets_all:Ts_batch,\n",
    "                    model.sen_len:x_len_batch,\n",
    "                    model.target_len:T_len_batch,\n",
    "                    model.targets_all_len_a:Ts_len_batch,\n",
    "                    model.relate_self:R_Self_batch,\n",
    "                    model.relate_cross:R_Cross_batxh,\n",
    "                    model.target_which: T_W_batch,\n",
    "                    model.target_position: T_P_batch,\n",
    "                    model.targets_all_position_a: Ts_P_batch,\n",
    "                    model.input_y: y_batch,\n",
    "                    model.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy, softmax,true_y,predictions = sess.run(\n",
    "                    [global_step, summary, model.loss, model.accuracy, model.softmax,model.true_y, model.predictions],\n",
    "                    feed_dict)\n",
    "                F1 = metrics.f1_score(true_y, predictions, average='macro')\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                log(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy,F1))\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy,F1))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "                return accuracy,softmax,F1\n",
    "\n",
    "            # Generate batches\n",
    "            batches = data_helpers.batch_iter(\n",
    "                list(zip(Train['x'],Train['T'],Train['Ts'],Train['x_len'], Train['T_len'], Train['Ts_len'],\n",
    "                         Train['R_Self'],Train['R_Cross'],Train['T_W'],Train['T_P'],Train['Ts_P'],Train['y'])), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            # Training loop. For each batch...\n",
    "            train_acc, dev_acc, test_acc, train_all_softmax, test_all_softmax = [], [], [], [], []\n",
    "            max_test_acc = 0\n",
    "            max_test_F1_macro = 0\n",
    "            for batch in batches:\n",
    "                x_batch,T_batch,Ts_batch,x_len_batch,T_len_batch,Ts_len_batch,R_Self_batch,R_Cross_batxh,T_W_batch,T_P_batch,Ts_P_batch,y_batch = zip(*batch)\n",
    "                train_step(x_batch,T_batch,Ts_batch,x_len_batch,T_len_batch,Ts_len_batch,R_Self_batch,R_Cross_batxh,T_W_batch,T_P_batch,Ts_P_batch,y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    log('\\nBy now ,the max test acc is: ')\n",
    "                    print('\\nBy now ,the max test acc is: ', max_test_acc)\n",
    "                    log('        the max F1 score is: ')\n",
    "                    print('        the max F1 score is: ', max_test_F1_macro)\n",
    "                    log(\"\\nEvaluation Text:\")\n",
    "                    print(\"\\nEvaluation Text:\")\n",
    "                    test_acc_i, test_softmax_i, test_F1_i = test_step(Test['x'],Test['T'],Test['Ts'],Test['x_len'], Test['T_len'], Test['Ts_len'],\n",
    "                                                           Test['R_Self'],Test['R_Cross'],Test['T_W'],Test['T_P'],Test['Ts_P'],Test['y'], summary = test_summary_op, writer=test_summary_writer)\n",
    "                    test_acc.append(test_acc_i)\n",
    "                    test_all_softmax.append(test_softmax_i)\n",
    "                    log('----------------------------------------------------------')\n",
    "                    print('----------------------------------------------------------')\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    if test_acc_i>max_test_acc:\n",
    "                        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                        log(\"Saved model checkpoint to {}\\n\".format(path)+'\\n'+'->>>>>>>>>>>>>>>>>>>>>>>')\n",
    "                        print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                        print('->>>>>>>>>>>>>>>>>>>>>>>')\n",
    "                        max_test_step = current_step\n",
    "                        max_test_acc = test_acc_i\n",
    "                    if test_F1_i > max_test_F1_macro:\n",
    "                        max_test_F1_macro = test_F1_i\n",
    "            log('max_test_step: ' + max_test_step)\n",
    "            print('max_test_step: ', max_test_step)\n",
    "            log('max_test_acc: ' + max_test_acc)\n",
    "            print('max_test_acc: ', max_test_acc)\n",
    "            log('max_test_F1_macro: ' + max_test_F1_macro)\n",
    "            print('max_test_F1_macro: ', max_test_F1_macro)\n",
    "    return train_acc, dev_acc, max_test_acc,max_test_F1_macro,max_test_step, train_all_softmax, test_all_softmax\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t#模型训练\n",
    "\ttrain_acc, dev_acc, max_test_acc,max_test_F1_macro,max_test_step, train_all_softmax, test_all_softmax = train(Train, Test, word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c7143d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4587, 300)\n",
      "4586 4587\n"
     ]
    }
   ],
   "source": [
    "w2v_file = 'data/data_res/300_rest14_embedding_matrix.pkl'\n",
    "embedding_dim = 300\n",
    "fp = open(w2v_file, 'rb')\n",
    "fp = cPickle.load(fp)\n",
    "if 0:\n",
    "    fp.readline()\n",
    "w2v = []\n",
    "word_dict = dict()\n",
    "# [0,0,...,0] represent absent words\n",
    "w2v.append([0.] * 300)\n",
    "cnt = 0\n",
    "for line in fp:\n",
    "    cnt += 1\n",
    "    # line = np.array2string(line)\n",
    "    # line = line.split()\n",
    "#     if len(line) != embedding_dim + 1: #3411,3798,4207\n",
    "#         print('a bad word embedding: {}'.format(line[0]))\n",
    "#         cnt -= 1\n",
    "#         continue\n",
    "    # w2v.append([float(v) for v in line[1:]])\n",
    "    w2v.append(line)\n",
    "    word_dict[line[0]] = cnt\n",
    "w2v = np.asarray(w2v, dtype=np.float32)\n",
    "w2v = np.row_stack((w2v, np.sum(w2v, axis=0) / cnt))\n",
    "print (np.shape(w2v))\n",
    "word_dict['UNK'] = cnt + 1\n",
    "print(word_dict['UNK'], len(w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19aef86d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute '_load_specials'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/whusdg/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/whusdg/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \"\"\"\n\u001b[0;32m-> 1230\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/whusdg/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \"\"\"\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/whusdg/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute '_load_specials'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0abb9d20cd42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw2v_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/data_res/300_rest14_embedding_matrix.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/whusdg/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/whusdg/lib/python3.6/site-packages/gensim/models/deprecated/word2vec.py\u001b[0m in \u001b[0;36mload_old_word2vec\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mold_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vector_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     params = {\n",
      "\u001b[0;32m~/opt/anaconda3/envs/whusdg/lib/python3.6/site-packages/gensim/models/deprecated/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m         \u001b[0;31m# update older models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/whusdg/lib/python3.6/site-packages/gensim/models/deprecated/old_saveload.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute '_load_specials'"
     ]
    }
   ],
   "source": [
    "w2v_file = 'data/data_res/300_rest14_embedding_matrix.pkl'\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a61049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
